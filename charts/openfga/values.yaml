replicaCount: 3

image:
  repository: openfga/openfga
  pullPolicy: Always
  tag: ""

imagePullSecrets: []
nameOverride: ""
fullnameOverride: ""

# Specify additional labels to apply to OpenFGA resources
# Usage example:
# commonLabels:
#     app.example.com/system: permissions
#     app.example.com/domain: example
commonLabels: {}

serviceAccount:
  # Specifies whether a service account should be created
  create: true
  # Annotations to add to the service account
  annotations: {}
  # The name of the service account to use.
  # If not set and create is true, a name is generated using the fullname template
  name: ""

annotations: {}

podAnnotations: {}
podExtraLabels: {}

extraEnvVars: []
extraVolumes: []
extraVolumeMounts: []
extraInitContainers: []

podSecurityContext:
  {}
  # fsGroup: 2000

securityContext:
  {}
  # capabilities:
  #   drop:
  #   - ALL
  # readOnlyRootFilesystem: true
  # runAsNonRoot: true
  # runAsUser: 1000

initContainer:
  repository: groundnuty/k8s-wait-for
  tag: "v2.0"
  pullPolicy: IfNotPresent

## Configure extra options for OpenFGA containers' liveness, readiness and startup probes
## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/#configure-probes

## @param livenessProbe.enabled Enable liveness probes on OpenFGA containers.
## @param livenessProbe.initialDelaySeconds Number of seconds after the container has started before liveness probes are initiated.
## @param livenessProbe.periodSeconds How often (in seconds) to perform the probe.
## @param livenessProbe.timeoutSeconds Number of seconds after which the probe times out.
## @param livenessProbe.failureThreshold Failure threshold for liveness probes.
## @param livenessProbe.successThreshold Success threshold for liveness probes.
##
livenessProbe:
  enabled: true
  initialDelaySeconds: 60
  periodSeconds: 10
  timeoutSeconds: 5
  failureThreshold: 12
  successThreshold: 1

## @param readinessProbe.enabled Enable readiness probes on OpenFGA containers.
## @param readinessProbe.initialDelaySeconds Number of seconds after the container has started before readiness probes are initiated.
## @param readinessProbe.periodSeconds How often (in seconds) to perform the probe.
## @param readinessProbe.timeoutSeconds Number of seconds after which the probe times out.
## @param readinessProbe.failureThreshold Failure threshold for readiness probes.
## @param readinessProbe.successThreshold Success threshold for readiness probes.
##
readinessProbe:
  enabled: true
  initialDelaySeconds: 5
  periodSeconds: 10
  timeoutSeconds: 5
  failureThreshold: 6
  successThreshold: 1

## @param startupProbe.enabled Enable startup probes on OpenFGA containers.
## @param startupProbe.initialDelaySeconds Number of seconds after the container has started before startup probes are initiated.
## @param startupProbe.periodSeconds How often (in seconds) to perform the probe.
## @param startupProbe.timeoutSeconds Number of seconds after which the probe times out.
## @param startupProbe.failureThreshold Failure threshold for startup probes.
## @param startupProbe.successThreshold Success threshold for startup probes.
##
startupProbe:
  enabled: false
  initialDelaySeconds: 60
  periodSeconds: 10
  timeoutSeconds: 5
  failureThreshold: 30
  successThreshold: 1

## @param customLivenessProbe Overrides the default liveness probe with a custom one.
##
customLivenessProbe: {}

## @param customReadinessProbe Overrides the default readiness probe with a custom one.
##
customReadinessProbe: {}

## @param customStartupProbe Overrides the default startup probe with a custom one.
##
customStartupProbe: {}

service:
  annotations: {}
  type: ClusterIP
  port: 8080

telemetry:
  trace:
    enabled: false
    otlp:
      endpoint:
      tls:
        enabled: false
    sampleRatio:

  metrics:
    ## @param telemetry.metrics.enabled enable/disable prometheus metrics on the '/metrics' endpoint
    ##
    enabled: true

    serviceMonitor:
      ## @param telemetry.metrics.serviceMonitor.enabled enable/disable installation of serviceMonitor custom resource
      ##
      enabled: false

      ## @param telemetry.metrics.serviceMonitor.additionalLabels additional labels to be added to the serivceMonitor resource
      ##
      additionalLabels: {}

      ## @param telemetry.metrics.serviceMonitor.annotations annotations to be added to the serviceMonitor resource
      ##
      annotations: {}

      ## @param telemetry.metrics.serviceMonitor.jobLabel the label to use to retrieve the job name from
      ##
      jobLabel: "app.kubernetes.io/name"

      ## @param telemetry.metrics.serviceMonitor.namespace namespace where the serviceMonitor resource should be installed to
      ##
      namespace: ""

      ## @param telemetry.metrics.serviceMonitor.namespaceSelector which namespaces should be scraped
      ##
      ## Default: scrape .Release.Namespace or namespaceOverride only
      ## To scrape all, use the following:
      ## namespaceSelector:
      ##   any: true
      ##
      namespaceSelector: {}

      ## @param telemetry.metrics.serviceMonitor.scrapeInterval prometheus scrape interval
      ##
      scrapeInterval: 30s

      ## @param telemetry.metrics.serviceMonitor.scrapeTimeout prometheus scrape timeout
      ##
      scrapeTimeout: 10s

      ## @param telemetry.metrics.serviceMonitor.targetLabels additional target labels to scrape
      ##
      targetLabels: []

      ## @param telemetry.metrics.serviceMonitor.relabelings add job relabelings
      ##
      relabelings: []

      ## @param telemetry.metrics.serviceMonitor.metricRelabelings add metric relabelings
      ##
      metricRelabelings: []

    ## @param telemetry.metrics.addr the host:port address to serve the Metrics server on
    addr: 0.0.0.0:2112

    ## @param telemetry.metrics.enableRPCHistograms enables prometheus histogram metrics for RPC latency distributions
    enableRPCHistograms:

    ## @param telemetry.metrics.podAnnotations [object] Annotations for the Prometheus metrics on etcd pods
    ##
    podAnnotations:
      prometheus.io/scrape: "true"
      prometheus.io/port: "{{ .Values.containerPorts.prometheus }}"

datastore:
  engine: memory
  uri:
  uriSecret:
  maxCacheSize:
  maxOpenConns:
  maxIdleConns:
  connMaxIdleTime:
  connMaxLifetime:
  applyMigrations: true
  waitForMigrations: true
  migrations:
    resources: {}
    image:
      repository: groundnuty/k8s-wait-for
      pullPolicy: Always
      tag: "v2.0"

postgresql:
  ## @param postgresql.enabled enable the bitnami/postgresql subchart and deploy Postgres
  enabled: false

mysql:
  ## @param mysql.enabled enable the bitnami/mysql subchart and deploy MySQL
  enabled: false

grpc:
  addr: 0.0.0.0:8081
  tls:
    enabled: false
    cert:
    key:

http:
  enabled: true
  addr: 0.0.0.0:8080
  tls:
    enabled: false
    cert:
    key:
  upstreamTimeout:
  corsAllowedOrigins: ["*"]
  corsAllowedHeaders: ["*"]

authn:
  method:
  preshared:
    keys: []
  oidc:
    audience:
    issuer:

playground:
  enabled: true
  port: 3000

profiler:
  enabled: false
  addr: 0.0.0.0:3001

log:
  level: info
  format: json
  timestampFormat: Unix

checkQueryCache:
  enabled: false
  limit:
  ttl:

experimentals: []

maxTuplesPerWrite:
maxTypesPerAuthorizationModel:
maxAuthorizationModelSizeInBytes:
maxConcurrentReadsForCheck:
maxConcurrentReadsForListObjects:
maxConcurrentReadsForListUsers:
changelogHorizonOffset:
resolveNodeLimit:
resolveNodeBreadthLimit:
listObjectsDeadline:
listObjectsMaxResults:
listUsersDeadline:
listUsersMaxResults:
requestDurationDatastoreQueryCountBuckets: [50, 200]
allowWriting1_0Models:
allowEvaluating1_0Models:

ingress:
  enabled: false
  className: ""
  annotations:
    {}
    # kubernetes.io/ingress.class: nginx
    # kubernetes.io/tls-acme: "true"
  hosts:
    - host: chart-example.local
      paths:
        - path: /
          pathType: ImplementationSpecific
          # servicePort: 8080
  tls: []
  #  - secretName: chart-example-tls
  #    hosts:
  #      - chart-example.local

resources: {}

autoscaling:
  enabled: false
  minReplicas: 1
  maxReplicas: 100
  targetCPUUtilizationPercentage: 80
  targetMemoryUtilizationPercentage: 80

nodeSelector: {}

tolerations: []

affinity: {}

# @param sidecars Add additional sidecar containers to the pods
# Example:
# sidecars:
#   - name: your-image-name
#     image: your-image
#     imagePullPolicy: Always
#     ports:
#       - name: portname
#         containerPort: 1234
sidecars: []
migrate:
  extraVolumes: []
  extraVolumeMounts: []
  sidecars: []
  annotations:
    helm.sh/hook: "post-install, post-upgrade, post-rollback, post-delete"
    helm.sh/hook-weight: "-5"
    helm.sh/hook-delete-policy: "before-hook-creation"
  labels: {}
  timeout:

# -- Array of extra K8s manifests to deploy
## Note: Supports use of custom Helm templates
extraObjects: []
## Example: Deploying a CloudnativePG Postgres cluster for use with OpenFGA:
# - apiVersion: postgresql.cnpg.io/v1
#   kind: Cluster
#   metadata:
#     name: openfga
#   spec:
#     instances: 3
#     storage:
#       size: 10Gi
